{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72362e01",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c9459d75",
   "metadata": {},
   "source": [
    "In general machine learning has inputs of 4 types,\n",
    "    -> Numbers\n",
    "    -> Text\n",
    "    -> Videos\n",
    "    -> Images\n",
    "    -> Sensors\n",
    "    \n",
    "From these types, we are taking Text for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54e04513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aa0705a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "210295fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"At eight o'clock on Thursday morning Arthur didn't feel very good.\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1612907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b2fe838",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "040bbef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3304ad86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0149170e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text =  \"hi this is Archana's mobile phone. She is having iphone 11\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ac48554",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Archana's mobile phone. She is having iphone 11\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8dc2a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'this',\n",
       " 'is',\n",
       " 'Archana',\n",
       " \"'s\",\n",
       " 'mobile',\n",
       " 'phone',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'having',\n",
       " 'iphone',\n",
       " '11']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "404d5278",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8d146ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'this',\n",
       " 'is',\n",
       " 'Archana',\n",
       " \"'s\",\n",
       " 'mobile',\n",
       " 'phone',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'having',\n",
       " 'iphone',\n",
       " '11']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "47379ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n",
      "this\n",
      "is\n",
      "Archana\n",
      "'s\n",
      "mobile\n",
      "phone\n",
      ".\n",
      "She\n",
      "is\n",
      "having\n",
      "iphone\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "for i in tokens:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe24472f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('hi', 'NN'),\n",
       " ('this', 'DT'),\n",
       " ('is', 'VBZ'),\n",
       " ('Archana', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('mobile', 'JJ'),\n",
       " ('phone', 'NN'),\n",
       " ('.', '.'),\n",
       " ('She', 'PRP'),\n",
       " ('is', 'VBZ'),\n",
       " ('having', 'VBG'),\n",
       " ('iphone', 'RB'),\n",
       " ('11', 'CD')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3f25efdd",
   "metadata": {},
   "source": [
    "POS Tagging (Parts of Speech Tagging) is a process to mark up the words in text format for a particular part of a speech based on its definition and context. It is responsible for text reading in a language and assigning some specific token (Parts of Speech) to each word. It is also called grammatical tagging."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ee2e9759",
   "metadata": {},
   "source": [
    "CC coordinating conjunction \n",
    "CD cardinal digit \n",
    "DT determiner \n",
    "EX existential there (like: “there is” … think of it like “there exists”) \n",
    "FW foreign word \n",
    "IN preposition/subordinating conjunction \n",
    "JJ adjective – ‘big’ \n",
    "JJR adjective, comparative – ‘bigger’ \n",
    "JJS adjective, superlative – ‘biggest’ \n",
    "LS list marker 1) \n",
    "MD modal – could, will \n",
    "NN noun, singular ‘- desk’ \n",
    "NNS noun plural – ‘desks’ \n",
    "NNP proper noun, singular – ‘Harrison’ \n",
    "NNPS proper noun, plural – ‘Americans’ \n",
    "PDT predeterminer – ‘all the kids’ \n",
    "POS possessive ending parent’s \n",
    "PRP personal pronoun –  I, he, she \n",
    "PRP$ possessive pronoun – my, his, hers \n",
    "RB adverb – very, silently, \n",
    "RBR adverb, comparative – better \n",
    "RBS adverb, superlative – best \n",
    "RP particle – give up \n",
    "TO – to go ‘to’ the store. \n",
    "UH interjection – errrrrrrrm \n",
    "VB verb, base form – take \n",
    "VBD verb, past tense – took \n",
    "VBG verb, gerund/present participle – taking \n",
    "VBN verb, past participle – taken \n",
    "VBP verb, sing. present, non-3d – take \n",
    "VBZ verb, 3rd person sing. present – takes \n",
    "WDT wh-determiner – which \n",
    "WP wh-pronoun – who, what \n",
    "WP$ possessive wh-pronoun, eg- whose \n",
    "WRB wh-adverb, eg- where, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c360dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eebc9007",
   "metadata": {},
   "source": [
    "The Treebank tokenizer uses regular expressions to tokenize text as in Penn Treebank.\n",
    "\n",
    "This tokenizer performs the following steps:\n",
    "\n",
    "split standard contractions, e.g. don't -> do n't and they'll -> they 'll\n",
    "\n",
    "treat most punctuation characters as separate tokens\n",
    "\n",
    "split off commas and single quotes, when followed by whitespace\n",
    "\n",
    "separate periods that appear at the end of line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3a632219",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok=TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b717422e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<nltk.tokenize.treebank.TreebankWordTokenizer at 0x14fd491dad0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d36e49fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'this',\n",
       " 'is',\n",
       " 'Archana',\n",
       " \"'s\",\n",
       " 'mobile',\n",
       " 'phone.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'having',\n",
       " 'iphone',\n",
       " '11']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6e62d393",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import PunktSentenceTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4344783d",
   "metadata": {},
   "source": [
    "A sentence tokenizer which uses an unsupervised algorithm to build a model for abbreviation words, collocations, and words that start sentences; and then uses that model to find sentence boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "248253fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = PunktSentenceTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b001bcb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hi this is Archana's mobile phone.\", 'She is having iphone 11']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a66712ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "raw",
   "id": "d80379d6",
   "metadata": {},
   "source": [
    "we are able to extract the tokens from string of words or sentences in the form of Alphabetic and Non-Alphabetic character by using tokenize.WordPunctTokenizer()() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73448391",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = WordPunctTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b2d32e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gfg = \"GeeksforGeeks...$$&* \\nis\\t for geeks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "2cd4d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "geek = tok.tokenize(gfg) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0d0cf1f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GeeksforGeeks', '...$$&*', 'is', 'for', 'geeks']"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "geek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b327c0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hi',\n",
       " 'this',\n",
       " 'is',\n",
       " 'Archana',\n",
       " \"'\",\n",
       " 's',\n",
       " 'mobile',\n",
       " 'phone',\n",
       " '.',\n",
       " 'She',\n",
       " 'is',\n",
       " 'having',\n",
       " 'iphone',\n",
       " '11']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e711276b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f0e16624",
   "metadata": {},
   "source": [
    "WordNet is the lexical database i.e. dictionary for the English language, specifically designed for natural language processing. \n",
    "\n",
    "Synset is a special kind of a simple interface that is present in NLTK to look up words in WordNet. Synset instances are the groupings of synonymous words that express the same concept. Some of the words have only one Synset and some have several. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b326b37c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('beautiful.a.01'), Synset('beautiful.s.02')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"Beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b17bf0d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('beautiful.a.01')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets(\"Beautiful\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d81d9d41",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn=wordnet.synsets(\"Beautiful\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f038f99c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'delighting the senses or exciting intellectual or emotional admiration'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "56e11353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('computer.n.01'), Synset('calculator.n.01')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.synsets('Computer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d60b86f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "syn=wordnet.synsets(\"Computer\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3c0685fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a machine for performing calculations automatically'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.definition()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "115d248f",
   "metadata": {},
   "source": [
    "Hypernyms are words that represent a category or a general concept that other words can fall under. For example, “fruit” is a hypernym for “apple”, “banana”, “orange”, and so on.\n",
    "\n",
    "Hyponyms, on the other hand, are words that fall under a particular category or concept. For example, “apple”, “banana”, and “orange” are hyponyms of the hypernym “fruit”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "079fd212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('machine.n.01')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hypernyms() # Generic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "39eaca54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "84c3d937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Synset('analog_computer.n.01'),\n",
       " Synset('digital_computer.n.01'),\n",
       " Synset('home_computer.n.01'),\n",
       " Synset('node.n.08'),\n",
       " Synset('number_cruncher.n.02'),\n",
       " Synset('pari-mutuel_machine.n.01'),\n",
       " Synset('predictor.n.03'),\n",
       " Synset('server.n.03'),\n",
       " Synset('turing_machine.n.01'),\n",
       " Synset('web_site.n.01')]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.hyponyms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "740aacc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.pos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f32c359e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Synset('computer.n.01')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7cbdd6b0",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6aa1f105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Lemma('computer.n.01.computer'),\n",
       " Lemma('computer.n.01.computing_machine'),\n",
       " Lemma('computer.n.01.computing_device'),\n",
       " Lemma('computer.n.01.data_processor'),\n",
       " Lemma('computer.n.01.electronic_computer'),\n",
       " Lemma('computer.n.01.information_processing_system')]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemmas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9393d5c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['computer',\n",
       " 'computing_machine',\n",
       " 'computing_device',\n",
       " 'data_processor',\n",
       " 'electronic_computer',\n",
       " 'information_processing_system']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syn.lemma_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0e1307b1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e01f1b75",
   "metadata": {},
   "source": [
    "TextBlob returns polarity and subjectivity of a sentence. Polarity lies between [-1,1], -1 defines a negative sentiment and 1 defines a positive sentiment. Negation words reverse the polarity."
   ]
  },
  {
   "cell_type": "raw",
   "id": "bd92dfe3",
   "metadata": {},
   "source": [
    "TextBlob is a simple library which supports complex analysis and operations on textual data.\n",
    "\n",
    "For lexicon-based approaches, a sentiment is defined by its semantic orientation and the intensity of each word in the sentence. This requires a pre-defined dictionary classifying negative and positive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cb505946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblobNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/636.8 kB ? eta -:--:--\n",
      "     --- ----------------------------------- 61.4/636.8 kB 1.6 MB/s eta 0:00:01\n",
      "     --- ----------------------------------- 61.4/636.8 kB 1.6 MB/s eta 0:00:01\n",
      "     ---- -------------------------------- 71.7/636.8 kB 558.5 kB/s eta 0:00:02\n",
      "     ----- ------------------------------- 92.2/636.8 kB 521.8 kB/s eta 0:00:02\n",
      "     ----- ------------------------------- 92.2/636.8 kB 521.8 kB/s eta 0:00:02\n",
      "     ----- ------------------------------ 102.4/636.8 kB 420.8 kB/s eta 0:00:02\n",
      "     ------ ----------------------------- 122.9/636.8 kB 379.3 kB/s eta 0:00:02\n",
      "     -------- --------------------------- 153.6/636.8 kB 435.7 kB/s eta 0:00:02\n",
      "     ---------- ------------------------- 184.3/636.8 kB 445.2 kB/s eta 0:00:02\n",
      "     ----------- ------------------------ 204.8/636.8 kB 444.2 kB/s eta 0:00:01\n",
      "     ------------ ----------------------- 225.3/636.8 kB 458.5 kB/s eta 0:00:01\n",
      "     -------------- --------------------- 256.0/636.8 kB 449.3 kB/s eta 0:00:01\n",
      "     ---------------- ------------------- 286.7/636.8 kB 465.5 kB/s eta 0:00:01\n",
      "     ----------------- ------------------ 317.4/636.8 kB 479.3 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 337.9/636.8 kB 476.3 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 348.2/636.8 kB 480.0 kB/s eta 0:00:01\n",
      "     --------------------- -------------- 389.1/636.8 kB 494.6 kB/s eta 0:00:01\n",
      "     ----------------------- ------------ 419.8/636.8 kB 494.1 kB/s eta 0:00:01\n",
      "     ------------------------ ----------- 430.1/636.8 kB 488.2 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 450.6/636.8 kB 485.5 kB/s eta 0:00:01\n",
      "     -------------------------- --------- 471.0/636.8 kB 491.3 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 501.8/636.8 kB 483.8 kB/s eta 0:00:01\n",
      "     ---------------------------- ------- 512.0/636.8 kB 479.2 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 553.0/636.8 kB 488.9 kB/s eta 0:00:01\n",
      "     ------------------------------- ---- 553.0/636.8 kB 488.9 kB/s eta 0:00:01\n",
      "     -------------------------------- --- 583.7/636.8 kB 476.3 kB/s eta 0:00:01\n",
      "     --------------------------------- -- 593.9/636.8 kB 472.6 kB/s eta 0:00:01\n",
      "     ---------------------------------- - 614.4/636.8 kB 465.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 636.8/636.8 kB 471.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\91701\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\91701\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\91701\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\91701\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in c:\\users\\91701\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\91701\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.1->textblob) (0.4.6)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39c0cac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "80ef2f0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"hi this is Archana's mobile phone. She is having iphone 11\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ec3c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob=TextBlob(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "058c7b54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"hi this is Archana's mobile phone. She is having iphone 11\")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df847e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Hola, este es el teléfono móvil de Archana. Ella está teniendo iPhone 11\")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.translate(from_lang=\"en\",to=\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "048871eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextBlob(\"Salut, c'est le téléphone mobile d'Archana. Elle a l'iPhone 11\")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.translate(from_lang=\"en\",to=\"fr\") # france"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5facdf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob.polarity?"
   ]
  },
  {
   "cell_type": "raw",
   "id": "644b14f9",
   "metadata": {},
   "source": [
    "Docstring:   Convert a string or number to a floating point number, if possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "004e7d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob.sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "8c18002c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"hi happy day to all\").sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6ebee73f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I will kill you\").sentiment.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "aaeebae9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(\"I love  you\").sentiment.polarity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
